{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danhj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danhj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openpyxl\n",
    "from text_processing import readJSON, common_grams, bag, get_gram_id,remove_rows_with_none_elements, strip_text\n",
    "from features import feat_cnt, constant\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB, ComplementNB,CategoricalNB\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import NMF\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc(n, k):\n",
    "    \"\"\"\n",
    "    Return the number truncated to k decimal places, e.g, k = 0 returns only the integer part\n",
    "    \"\"\"\n",
    "\n",
    "    return int(n * 10**k) / 10**k\n",
    "\n",
    "def measure(y_true, y_pred, k):\n",
    "    \"\"\"\n",
    "    y_pred = binary vector\n",
    "    y_true = binary vector\n",
    "\n",
    "    Desc: Computes the accuracy, TP, TN, FP, FN, BER truncated to k decimal places\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(\"Lengths not equal: y_pred={}, y_true={}\".format(len(y_pred), len(y_true)))\n",
    "        return\n",
    "\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0,\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_true[i] and y_true[i] == 1:\n",
    "            TP += 1\n",
    "        elif y_pred[i] == y_true[i] and y_true[i] == 0:\n",
    "            TN += 1\n",
    "        elif y_pred[i] != y_true[i] and y_true[i] == 1:\n",
    "            FN += 1\n",
    "        elif y_pred[i] != y_true[i] and y_true[i] == 0:\n",
    "            FP += 1 \n",
    "    \n",
    "    total = TP + TN + FP + FN\n",
    "    print(total)\n",
    "    FPR = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    FPN = TN / (TN + FN) if TN + FN > 0 else 0\n",
    "    acc = (TP + TN) / total\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    BER = (FPR + FPN) / 2\n",
    "    print(f_score, acc)\n",
    "    return (TP, TN, FP, FN, trunc(precision, k), trunc(recall, k), trunc(acc, k), trunc(f_score, k))\n",
    "\n",
    "def prod(S):\n",
    "    product = 1\n",
    "    for x in S:\n",
    "        product *= x\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Excel file\n",
    "excel_file_path = 'Book1.xlsx'\n",
    "workbook = openpyxl.load_workbook(excel_file_path)\n",
    "\n",
    "# Select the active sheet (you can also select a specific sheet by name: workbook['Sheet1'])\n",
    "sheet = workbook.active\n",
    "\n",
    "# Create empty lists to store data from each column\n",
    "columns = [[] for _ in range(sheet.max_column)]\n",
    "counter = 0\n",
    "# Iterate through each row and store data from each column\n",
    "for row in sheet.iter_rows(values_only=True):\n",
    "    counter +=1\n",
    "    for i, cell_value in enumerate(row):\n",
    "        \n",
    "        columns[i].append(cell_value)\n",
    "\n",
    "\n",
    "# Close the workbook\n",
    "workbook.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    # Stem words\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = remove_rows_with_none_elements(columns)\n",
    "train_data = []\n",
    "train_data.append(columns[0][0:2587])\n",
    "train_data.append(columns[1][0:2587])\n",
    "\n",
    "test_data = []\n",
    "test_data.append(columns[0][2587:])\n",
    "test_data.append(columns[1][2587:])\n",
    "\n",
    "for i in train_data[0]:\n",
    "    i = clean_text(i)\n",
    "for i in test_data[0]:\n",
    "    i = clean_text(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dominant_topics(dataset1, dataset2):\n",
    "    \"\"\"\n",
    "    Create dominant topics using NMF factorization on two datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset1 (list): List of documents for the first dataset.\n",
    "        dataset2 (list): List of documents for the second dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists of dominant topics for each dataset.\n",
    "    \"\"\"\n",
    "    max_length = len(dataset2)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataset1)\n",
    "    tfidf_matrix2 = tfidf_vectorizer.fit_transform(dataset2)\n",
    "    # NMF factorization\n",
    "    num_topics = 10\n",
    "    nmf_model = NMF(n_components=num_topics, random_state=27)\n",
    "    nmf_model.fit(tfidf_matrix)\n",
    "    \n",
    "    dominant_topics = np.argsort(nmf_model.transform(tfidf_matrix), axis=1)[:, ::-1][:, :5]\n",
    "    \n",
    "    nmf_model.fit(tfidf_matrix2)\n",
    "    dominant_topics2 = np.argsort(nmf_model.transform(tfidf_matrix2), axis=1)[:, ::-1][:, :5]\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "        top_words_idx = topic.argsort()[-30:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "    # Print top words for each topic\n",
    "\n",
    "    res = []\n",
    "    for i in dominant_topics:\n",
    "        res.append(i)\n",
    "    \n",
    "    res2 = []\n",
    "    for i in dominant_topics2:\n",
    "        res2.append(i)  \n",
    "    \n",
    "    return res, res2, dominant_topics, dominant_topics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ve, time, like, day, got, week, just, little, ll, going, good, really, today, didn, did, year, new, said, home, know, make, days, food, bit, night, lot, think, went, friends, work\n",
      "Topic 2: nbsp, therizinosaurus, therizinosaurs, therizinosaur, luis, art, lesotho, skywatch, perez, live, just, subscribing, evolved, ichthyosaurs, world, time, like, shop, blog, clutch, skies, blogged, spring, farm, orthodox, ichthyosaur, did, think, thanks, church\n",
      "Topic 3: abt, ur, person, da, dat, wat, frnd, guy, best, really, dear, ppl, dont, class, tat, wen, like, hav, gud, lik, ma, friend, friends, say, testimonial, ya, wud, knw, testi, close\n",
      "Topic 4: new, blog, com, site, camera, great, mso, post, work, music, book, like, photo, use, francisco, san, web, page, photos, www, http, check, health, project, magazine, company, time, color, years, 2010\n",
      "Topic 5: people, don, just, want, know, love, life, like, think, things, really, make, say, right, thing, feel, need, doing, live, maybe, ve, guy, sure, hard, look, person, dream, way, happy, mind\n",
      "Topic 6: god, friend, life, evil, blessed, pray, just, wonderful, things, bless, job, absence, prayed, step, steffi, grateful, control, book, parents, time, good, helps, gone, feel, hide, changed, mother, troble, sijo, wrong\n",
      "Topic 7: school, bully, puppy, loss, high, deal, really, rid, taken, night, people, left, scrape, retaliation, chucky, boyfriends, painfully, stomp, bullying, grieve, proclaimed, reminded, ashamed, wont, shadow, crying, grief, survived, shoe, entered\n",
      "Topic 8: carnival, presents, posted, writing, art, submission, guidelines, submit, street, haiku, easy, blog, saying, existential, famous, sarah, edition, fiction, kane, maxwelldb, ontario, livia, phenomenally, blackburne, kajoemanis, poet, poetry, story, submissions, poem\n",
      "Topic 9: mother, lambs, birth, father, barn, lilly, trauma, beautiful, little, girl, cake, lamb, did, life, labor, year, night, old, udder, nelly, bridgette, lambing, bunny, like, brought, morning, creature, messy, warm, story\n",
      "Topic 10: india, indian, game, team, movie, israel, band, play, cricket, year, batting, games, movies, players, playing, tv, song, better, best, played, match, watch, ipl, world, came, ball, england, country, won, young\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danhj\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grouped_words,grouped_words_test,dominate_topics1, dominate_topics2 = create_dominant_topics(train_data[0], test_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639\n",
      "0.4033970276008493 0.5602503912363067\n",
      "639\n",
      "0.5587301587301587 0.564945226917058\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model_2 = ComplementNB()\n",
    "y = [1 if d == 'M' else 0 for d in train_data[1]] \n",
    "model.fit(grouped_words, y)\n",
    "model_2.fit(grouped_words, y)\n",
    "\n",
    "y_pred = model.predict(grouped_words_test)\n",
    "y_pred2 = model_2.predict(grouped_words_test)\n",
    "\n",
    "\n",
    "y_test = [1 if d == 'M' else 0 for d in test_data[1]]\n",
    "\n",
    "grouped_words_regression = \"grouped_words_regression: (TP, TN, FP, FN, precision, recall, acc, f_score) = {}\".format(measure(y_test, y_pred, 5))\n",
    "grouped_words_NB = \"grouped_words_NB: (TP, TN, FP, FN, precision, recall, acc, f_score) = {}\".format(measure(y_test, y_pred2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_words_regression: (TP, TN, FP, FN, precision, recall, acc, f_score) = (95, 263, 72, 209, 0.56886, 0.3125, 0.56025, 0.40339)\n",
      "grouped_words_NB: (TP, TN, FP, FN, precision, recall, acc, f_score) = (176, 185, 150, 128, 0.53987, 0.57894, 0.56494, 0.55873)\n",
      "[[2 0 3 1 8]\n",
      " [2 8 3 9 7]\n",
      " [3 6 2 7 0]\n",
      " ...\n",
      " [5 3 9 8 7]\n",
      " [3 2 8 0 6]\n",
      " [8 3 2 0 7]]\n"
     ]
    }
   ],
   "source": [
    "print(grouped_words_regression)\n",
    "print(grouped_words_NB)\n",
    "print(dominate_topics1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
